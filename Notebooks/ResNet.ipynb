{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run if using tensorflow2.0+\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Add, Activation, InputLayer\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.optimizers import SGD, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Add, Activation\n",
    "from keras import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0.248030\n",
       "1     0.197173\n",
       "3     0.133722\n",
       "2     0.110785\n",
       "4     0.080682\n",
       "5     0.055575\n",
       "7     0.053003\n",
       "8     0.049037\n",
       "6     0.037622\n",
       "9     0.027224\n",
       "10    0.007147\n",
       "Name: open_channels, dtype: float64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('../Data/train.csv')\n",
    "df_train['open_channels'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(batch_num='all', zeros = True):\n",
    "    #create training data per batch\n",
    "    batch_size = 500000\n",
    "    if batch_num == 'all':\n",
    "        beg = 0\n",
    "        end = 499999999\n",
    "        print('Training on all data')\n",
    "    else:\n",
    "        batch = batch_num\n",
    "        beg = (batch-1)*batch_size \n",
    "        end = batch*batch_size - 1\n",
    "    df_batch = df_train[beg:end]\n",
    "    \n",
    "    # filter out the zero open_channels cases\n",
    "    if not zeros:\n",
    "        df_batch = df_batch.drop(df_batch[df_batch['open_channels']==0].index) #optional, added because model was always predicting 0\n",
    "        \n",
    "    # randomize \n",
    "    signal = np.array(df_batch.signal)\n",
    "    open_channels = np.array(df_batch.open_channels)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(signal,open_channels,test_size=0.25)\n",
    "    x_train = np.reshape(x_train, (-1,1))\n",
    "    y_train = np.reshape(y_train, (-1,1))\n",
    "    x_test = np.reshape(x_test, (-1,1))\n",
    "    y_test = np.reshape(y_test, (-1,1))\n",
    "    \n",
    "    # categorize outputs\n",
    "    enc = OneHotEncoder()\n",
    "    enc.fit_transform(y_train)\n",
    "    enc.fit_transform(y_test)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on all data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3],\n",
       "       [3],\n",
       "       [0],\n",
       "       ...,\n",
       "       [4],\n",
       "       [3],\n",
       "       [0]], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test = get_data()\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(y_train)\n",
    "enc.categories_;\n",
    "enc.transform(y_train)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet(layer_size=64, dropout=0, depth=1):\n",
    "    #model params\n",
    "    layer = layer_size\n",
    "    dropout = dropout\n",
    "    layers = depth\n",
    "    \n",
    "    #create model\n",
    "    inputs = Input(shape=(1,))\n",
    "\n",
    "    if depth > 0:\n",
    "        k = Dense(layer, activation='relu')(inputs)\n",
    "        k = Dense(layer, activation='relu')(k)\n",
    "        k = Dropout(dropout)(k)\n",
    "        block1 = Add()([k, inputs])\n",
    "        kl = Dense(20, activation='relu')(block1)\n",
    "        \n",
    "        if depth > 1:\n",
    "            k = Dense(layer, activation='relu')(block1)\n",
    "            k = Dense(layer, activation='relu')(k)\n",
    "            k = Dropout(dropout)(k)\n",
    "            block2 = Add()([k, block1])\n",
    "            kl = Dense(20, activation='relu')(block2)\n",
    "\n",
    "            if depth > 2:\n",
    "                k = Dense(layer, activation='relu')(block2)\n",
    "                k = Dense(layer, activation='relu')(k)\n",
    "                k = Dropout(dropout)(k)\n",
    "                block3 = Add()([k, block2])\n",
    "                kl = Dense(20, activation='relu')(block3)\n",
    "\n",
    "                if depth > 3:\n",
    "                    k = Dense(layer, activation='relu')(block3)\n",
    "                    k = Dense(layer, activation='relu')(k)\n",
    "                    k = Dropout(dropout)(k)\n",
    "                    block4 = Add()([k, block3])\n",
    "                    kl = Dense(20, activation='relu')(block4)\n",
    "                    \n",
    "                    if depth > 4:\n",
    "                        k = Dense(layer, activation='relu')(block4)\n",
    "                        k = Dense(layer, activation='relu')(k)\n",
    "                        k = Dropout(dropout)(k)\n",
    "                        block5 = Add()([k, block4])\n",
    "                        kl = Dense(20, activation='relu')(block5)\n",
    "                        \n",
    "                        if depth > 5:\n",
    "                            k = Dense(layer, activation='relu')(block5)\n",
    "                            k = Dense(layer, activation='relu')(k)\n",
    "                            k = Dropout(dropout)(k)\n",
    "                            block6 = Add()([k, block5])\n",
    "                            kl = Dense(20, activation='relu')(block6)\n",
    "                        \n",
    "                            if depth > 6:\n",
    "                                k = Dense(layer, activation='relu')(block6)\n",
    "                                k = Dense(layer, activation='relu')(k)\n",
    "                                k = Dropout(dropout)(k)\n",
    "                                block7 = Add()([k, block6])\n",
    "                                kl = Dense(20, activation='relu')(block7)\n",
    "                                \n",
    "                                if depth > 7:\n",
    "                                    k = Dense(layer, activation='relu')(block7)\n",
    "                                    k = Dense(layer, activation='relu')(k)\n",
    "                                    k = Dropout(dropout)(k)\n",
    "                                    block8 = Add()([k, block7])\n",
    "                                    kl = Dense(20, activation='relu')(block8)\n",
    "\n",
    "    outputs = Dense(units=11, activation='softmax')(kl)\n",
    "    \n",
    "    model = Model(inputs,outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 374999 samples, validate on 125000 samples\n",
      "Epoch 1/5\n",
      "374999/374999 [==============================] - 42s 111us/sample - loss: 0.0890 - accuracy: 0.9682 - val_loss: 0.0838 - val_accuracy: 0.9677\n",
      "Epoch 2/5\n",
      "374999/374999 [==============================] - 46s 122us/sample - loss: 0.0716 - accuracy: 0.9691 - val_loss: 0.0479 - val_accuracy: 0.9749\n",
      "Epoch 3/5\n",
      "374999/374999 [==============================] - 45s 121us/sample - loss: 0.0385 - accuracy: 0.9865 - val_loss: 0.0079 - val_accuracy: 0.9977\n",
      "Epoch 4/5\n",
      "374999/374999 [==============================] - 45s 121us/sample - loss: 0.0137 - accuracy: 0.9958 - val_loss: 0.0163 - val_accuracy: 0.9948\n",
      "Epoch 5/5\n",
      "374999/374999 [==============================] - 46s 121us/sample - loss: 0.0094 - accuracy: 0.9969 - val_loss: 0.0300 - val_accuracy: 0.9892\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x24e29b01fd0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test = get_data(1,zeros=True)\n",
    "model = resnet(layer_size=256, dropout=0.1, depth=8)\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='sgd',metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=10, validation_data=(x_test,y_test), batch_size=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 374999 samples, validate on 125000 samples\n",
      "Epoch 1/10\n",
      "374999/374999 [==============================] - 41s 109us/sample - loss: 0.0867 - accuracy: 0.9682 - val_loss: 0.0716 - val_accuracy: 0.9683\n",
      "Epoch 2/10\n",
      "374999/374999 [==============================] - 50s 133us/sample - loss: 0.0497 - accuracy: 0.9808 - val_loss: 0.0117 - val_accuracy: 0.9980\n",
      "Epoch 3/10\n",
      "374999/374999 [==============================] - 45s 120us/sample - loss: 0.0152 - accuracy: 0.9954 - val_loss: 0.0082 - val_accuracy: 0.9974\n",
      "Epoch 4/10\n",
      "374999/374999 [==============================] - 47s 125us/sample - loss: 0.0095 - accuracy: 0.9968 - val_loss: 0.0061 - val_accuracy: 0.9978\n",
      "Epoch 5/10\n",
      "374999/374999 [==============================] - 47s 125us/sample - loss: 0.0082 - accuracy: 0.9972 - val_loss: 0.0090 - val_accuracy: 0.9968\n",
      "Epoch 6/10\n",
      "374999/374999 [==============================] - 48s 127us/sample - loss: 0.0078 - accuracy: 0.9974 - val_loss: 0.0060 - val_accuracy: 0.9979\n",
      "Epoch 7/10\n",
      "374999/374999 [==============================] - 48s 128us/sample - loss: 0.0071 - accuracy: 0.9976 - val_loss: 0.0113 - val_accuracy: 0.9960\n",
      "Epoch 8/10\n",
      "374999/374999 [==============================] - 49s 130us/sample - loss: 0.0070 - accuracy: 0.9976 - val_loss: 0.0057 - val_accuracy: 0.9980\n",
      "Epoch 9/10\n",
      "374999/374999 [==============================] - 48s 129us/sample - loss: 0.0067 - accuracy: 0.9977 - val_loss: 0.0059 - val_accuracy: 0.9979\n",
      "Epoch 10/10\n",
      "374999/374999 [==============================] - 51s 137us/sample - loss: 0.0066 - accuracy: 0.9978 - val_loss: 0.0059 - val_accuracy: 0.9979\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of binary and continuous-multioutput targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-9ef819ceeb26>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0macc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jazzy\\envs\\738\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[1;31m# Compute accuracy for each possible representation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'multilabel'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jazzy\\envs\\738\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[1;32m---> 90\u001b[1;33m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and continuous-multioutput targets"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for batch in range(1,11):    \n",
    "    x_train,x_test,y_train,y_test = get_data(batch,zeros=True)\n",
    "    model = resnet(layer_size=256, dropout=0.1, depth=8)\n",
    "    model.compile(loss='sparse_categorical_crossentropy',optimizer='sgd',metrics=['accuracy'])\n",
    "    model.fit(x_train, y_train, epochs=10, validation_data=(x_test,y_test), batch_size=100, verbose=1)\n",
    "    \n",
    "    y_pred = model.predict(x_test)\n",
    "    score = accuracy_score(y_test, y_pred)\n",
    "    acc.append(score)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = get_data(zeros=True)\n",
    "model = resnet(layer_size=256, dropout=0.1, depth=8)\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='sgd',metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=10, validation_data=(x_test,y_test), batch_size=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2249994 samples, validate on 750000 samples\n",
      "Epoch 1/6\n",
      "2249994/2249994 [==============================] - 148s 66us/sample - loss: 0.4876 - accuracy: 0.8271 - val_loss: 0.4126 - val_accuracy: 0.8458\n",
      "Epoch 2/6\n",
      "2249994/2249994 [==============================] - 148s 66us/sample - loss: 0.3900 - accuracy: 0.8576 - val_loss: 0.3777 - val_accuracy: 0.8623\n",
      "Epoch 3/6\n",
      "2249994/2249994 [==============================] - 153s 68us/sample - loss: 0.3849 - accuracy: 0.8594 - val_loss: 0.3939 - val_accuracy: 0.8567\n",
      "Epoch 4/6\n",
      "2249994/2249994 [==============================] - 155s 69us/sample - loss: 0.3827 - accuracy: 0.8602 - val_loss: 0.3762 - val_accuracy: 0.8608\n",
      "Epoch 5/6\n",
      "2249994/2249994 [==============================] - 156s 69us/sample - loss: 0.3813 - accuracy: 0.8605 - val_loss: 0.3817 - val_accuracy: 0.8599\n",
      "Epoch 6/6\n",
      "2249994/2249994 [==============================] - 156s 69us/sample - loss: 0.3803 - accuracy: 0.8609 - val_loss: 0.3765 - val_accuracy: 0.8624\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x24e286b40b8>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create model for flat signal type (batches 1-6)\n",
    "x_tr1,x_t1,y_tr1,y_t1 = get_data(1)\n",
    "x_tr2,x_t2,y_tr2,y_t2 = get_data(2)\n",
    "x_tr3,x_t3,y_tr3,y_t3 = get_data(3)\n",
    "x_tr4,x_t4,y_tr4,y_t4 = get_data(4)\n",
    "x_tr5,x_t5,y_tr5,y_t5 = get_data(5)\n",
    "x_tr6,x_t6,y_tr6,y_t6 = get_data(6)\n",
    "x_train = np.concatenate((x_tr1,x_tr2,x_tr3,x_tr4,x_tr5,x_tr6))\n",
    "x_test = np.concatenate((x_t1,x_t2,x_t3,x_t4,x_t5,x_t6))\n",
    "y_train = np.concatenate((y_tr1,y_tr2,y_tr3,y_tr4,y_tr5,y_tr6))\n",
    "y_test = np.concatenate((y_t1,y_t2,y_t3,y_t4,y_t5,y_t6))\n",
    "\n",
    "model1 = resnet(layer_size=175, dropout=0.1, depth=4)\n",
    "opt = Adam(learning_rate=.01,beta_1=0.95)\n",
    "model1.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "model1.fit(x_train, y_train, epochs=6, validation_data=(x_test,y_test), batch_size=80, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.save('../Models/resnet_flat_85.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1874995 samples, validate on 625000 samples\n",
      "Epoch 1/6\n",
      "1874995/1874995 [==============================] - 157s 84us/sample - loss: 1.6745 - accuracy: 0.3803 - val_loss: 1.4651 - val_accuracy: 0.4478\n",
      "Epoch 2/6\n",
      "1874995/1874995 [==============================] - 175s 93us/sample - loss: 1.3833 - accuracy: 0.4683 - val_loss: 1.3156 - val_accuracy: 0.4924\n",
      "Epoch 3/6\n",
      "1874995/1874995 [==============================] - 175s 94us/sample - loss: 1.3213 - accuracy: 0.4926 - val_loss: 1.3251 - val_accuracy: 0.4837\n",
      "Epoch 4/6\n",
      "1874995/1874995 [==============================] - 176s 94us/sample - loss: 1.3018 - accuracy: 0.4981 - val_loss: 1.3012 - val_accuracy: 0.5036\n",
      "Epoch 5/6\n",
      "1874995/1874995 [==============================] - 175s 93us/sample - loss: 1.2947 - accuracy: 0.5000 - val_loss: 1.2994 - val_accuracy: 0.4953\n",
      "Epoch 6/6\n",
      "1874995/1874995 [==============================] - 174s 93us/sample - loss: 1.2897 - accuracy: 0.5010 - val_loss: 1.2936 - val_accuracy: 0.4951\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x24e1e53cf60>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create model for parabolic signal type (batches 2,7-10)\n",
    "x_tr2,x_t2,y_tr2,y_t2 = get_data(2)\n",
    "x_tr7,x_t7,y_tr7,y_t7 = get_data(7)\n",
    "x_tr8,x_t8,y_tr8,y_t8 = get_data(8)\n",
    "x_tr9,x_t9,y_tr9,y_t9 = get_data(9)\n",
    "x_tr10,x_t10,y_tr10,y_t10 = get_data(10)\n",
    "x_train = np.concatenate((x_tr2,x_tr7,x_tr8,x_tr9,x_tr10))\n",
    "x_test = np.concatenate((x_t2,x_t7,x_t8,x_t9,x_t10))\n",
    "y_train = np.concatenate((y_tr2,y_tr7,y_tr8,y_tr9,y_tr10))\n",
    "y_test = np.concatenate((y_t2,y_t7,y_t8,y_t9,y_t10))\n",
    "\n",
    "model = resnet(layer_size=300, dropout=0.1, depth=2)\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='sgd',metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=6, validation_data=(x_test,y_test), batch_size=80, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../Models/resnet_curvy_49.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
